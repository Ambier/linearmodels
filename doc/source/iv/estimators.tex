%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{babel}
\begin{document}
\label{iv-mathematical-notation}

\section*{Mathematical Formulas}

\subsection*{Notation}

Interest is in recovering the parameter vector from the model 
\begin{align*}
y_{i} & =\beta^{\prime}x_{i}+\epsilon_{i}
\end{align*}

\noindent where $\beta$ is $k$ by 1, $x_{i}$ is a $k$ by 1 vector
of observable variables and $\epsilon_{i}$ is a scalar error. $x_{i}$
can be separated in two types of variables. The $k_{1}$ by 1 set
of variables $x_{1i}$ are exogenous regressors in the sense that
$E\left[x_{1i}\epsilon_{i}\right]=0$. The $k_{2}$ by 1 variables
$x_{2i}$ are endogenous. A set of $p$ instruments is available that
satisfy the requirements for validity where $p\geq k_{2}$. The extended
model can be written as 
\begin{align*}
y_{i} & =\underset{\textrm{exogenous}}{\underbrace{\beta_{1}^{\prime}x_{1i}}}+\underset{\textrm{endogenous}}{\underbrace{\beta_{2}^{\prime}x_{2i}}}+\epsilon_{i}\\
x_{2i} & =\underset{\textrm{exogenous}}{\underbrace{\gamma_{1}^{\prime}z_{1i}}}+\underset{\textrm{instruments}}{\underbrace{\gamma_{2}^{\prime}z_{2i}}}+u_{i}
\end{align*}

The model can be expressed compactly 
\begin{align*}
Y & =X_{1}\beta_{1}+X_{2}\beta_{1}+\epsilon=X\beta+\epsilon\\
X_{2} & =Z_{1}\gamma_{1}+Z_{2}\gamma_{2}+u=Z\gamma+u
\end{align*}

\noindent The vector of instruments $z_{i}$ is $p$ by 1. There are
$n$ observations for all variables. $k_{c}=1$ if the model contains
a constant (either explicit or implicit, i.e., including all categories
of a dummy variable). The constant, if included, is in $x_{1i}$.
$X$ is the $n$ by $k$ matrix if regressors where row $i$ of $X$
is $x_{i}^{\prime}$. $X$ can be partitioned into $\left[X_{1}\;X_{2}\right]$.
$Z$ is the $n$ by $p$ matrix of instruments. The vector $y$ is
$n$ by 1. Projection matrices for $X$ is defined $P_{X}=X\left(X^{\prime}X\right)^{-1}X^{\prime}$.
The projection matrix for $Z$ is similarly defined only using $Z$.
The annihilator matrix for $X$ is $M_{X}=I-P_{X}$.

\subsection*{Parameter Estimation}

\subsubsection*{Two-stage Least Squares (2SLS)}

The 2SLS estimator is 
\begin{align*}
\hat{\beta}_{2SLS} & =\left(X^{\prime}P_{Z}X\right)\left(X^{\prime}P_{Z}y\right)
\end{align*}


\subsubsection*{Limited Information Maximum Likelihood and k-class Estimators}

The LIML or other k-class estimator is 
\begin{align*}
\hat{\beta}_{\kappa} & =\left(X^{\prime}\left(I-\kappa M_{Z}\right)X\right)\left(X^{\prime}\left(I-\kappa M_{Z}\right)y\right)
\end{align*}

\noindent where $\kappa$ is the parameter of the class. When $\kappa=1$
the 2SLS estimator is recovered. When $\kappa=0$, the OLS estimator
is recovered. The LIML estimator is recovered when $\kappa$ set to
\[
\hat{\kappa}=\min\mathrm{eig\left[\left(W^{\prime}M_{z}W\right)^{-\frac{1}{2}}\left(W^{\prime}M_{X_{1}}W\right)\left(W^{\prime}M_{z}W\right)^{-\frac{1}{2}}\right]}
\]

\noindent where $W=\left[y\:X_{2}\right]$ and $\mathrm{eig}$ returns
the eigenvalues. 

\subsubsection*{Generalized Method of Moments (GMM)}

The GMM estimator is defined as 
\begin{align*}
\hat{\beta}_{GMM} & =\left(X^{\prime}ZWZ^{\prime}X\right)^{-1}\left(X^{\prime}ZWZ^{\prime}y\right)
\end{align*}

\noindent where $W$ is a positive definite weighting matrix. 

\subsubsection*{Continuously Updating Generalized Method of Moments (GMM-CUE)}

The continuously updating GMM estimator solves the minimization problem
\[
\min_{\beta}n\bar{g}\left(\beta\right)^{\prime}W\left(\beta\right)^{-1}\bar{g}\left(\beta\right)
\]
where $\bar{g}\left(\beta\right)=n^{-1}\sum_{i=1}^{n}z_{i}\hat{\epsilon}_{i}$
and $\hat{\epsilon}_{i}=y_{i}-x_{i}\beta$. Unlike standard GMM, the
weight matrix, $W$ directly depends on the model parameters $\beta$
and so it is not possible to use a closed form estimator. 

\subsection*{Basic Statistics}

Let $\hat{\epsilon}=y-X\hat{\beta}$. The residual sum of squares
(RSS) is $\hat{\epsilon}^{\prime}\hat{\epsilon}$, the model sum of
squares (MSS) is $\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}$ and
the total sum of squares (TSS) is $\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}$where
$\bar{y}$ is the sample average of $y$. The model $R^{2}$is defined
\begin{align*}
R^{2} & =1-\frac{\hat{\epsilon}^{\prime}\hat{\epsilon}}{\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}}=1-\frac{RSS}{TSS}
\end{align*}

\noindent and the adjusted $R^{2}$ is defined 
\begin{align*}
\bar{R}^{2} & =1-\left(1-R^{2}\right)\frac{N-k_{c}}{N-k}.
\end{align*}

\noindent The residual variance is $s^{2}=n^{-1}\hat{\epsilon}^{\prime}\hat{\epsilon}$
unless the debiased flag is used, in which case a small sample adjusted
version is estimated $s^{2}=\left(n-k\right)^{-1}\hat{\epsilon}^{\prime}\hat{\epsilon}$.

The model F-statistic is defined 
\begin{align*}
F & =\hat{\beta}_{-}^{\prime}\hat{V}_{-}^{-1}\dot{\hat{\beta}_{-}}
\end{align*}

\noindent where the notation $\hat{\beta}_{-}$ indicates that the
constant is excluded from $\hat{\beta}$ and $\hat{V}_{-}$ indicates
that the row and column corresponding to a constant are excluded.\footnote{If the model contains an implicit constant, e.g., all categories of
a dummy, one of the categories is excluded when computing the test
statistic. The choice of category to drop has no effect and is equivalent
to reparameterizing the model with a constant and excluding one category
of dummy.} The test statistic is distributed as $\chi_{k-k_{c}}^{2}.$ If the
debiased flag is set, then the test statistic $F$ is transformed
as $F/\left(k-k_{c}\right)$ and a $F_{k-k_{c},n-k}$ distribution
is used. 

\subsection*{Parameter Covariance Estimation}

\subsubsection*{Two-stage LS, LIML and k-class estimators}

Four covariance estimators are available. The first is the standard
homoskedastic covariance, defined as 

\begin{align*}
n^{-1}s^{2}\left(\frac{X^{\prime}\left(I-\kappa M_{z}\right)X}{n}\right)^{-1} & =n^{-1}s^{2}\hat{A}.
\end{align*}

Note that this estimator can be expressed as 
\begin{align*}
n^{-1}\hat{A}^{-1}\left\{ s^{2}\hat{A}\right\} \hat{A}^{-1} & =n^{-1}\hat{A}^{-1}\hat{B}\hat{A}^{-1}.
\end{align*}

\noindent All estimators take this form and only differ in how the
asymptotic covariance of the scores, $B$, is estimated. For the homoskedastic
covariance estimator, $\hat{B}=s^{2}\hat{A}.$ The score covariance
in the heteroskedasticity robust covariance estimator is 
\begin{align*}
\hat{B} & =n^{-1}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}\hat{x}_{i}\hat{x}_{i}^{\prime}=n^{-1}\sum_{i=1}^{n}\hat{\xi}_{i}\hat{\xi}_{i}^{\prime}.
\end{align*}

\noindent where $\hat{x_{i}}$ is row $i$ of $\hat{X}=P_{Z}X$ and
$\hat{\xi}_{i}=\hat{\epsilon}_{i}\hat{x}_{i}$.

The kernel covariance estimator is robust to both heteroskedasticity
and autocorrelation and is defined as 
\begin{align*}
\hat{B} & =\hat{\Gamma}_{0}+\sum_{i=1}^{n-1}k\left(i/h\right)\left(\hat{\Gamma}_{i}+\hat{\Gamma}_{i}^{\prime}\right)\\
\hat{\Gamma_{j}} & =n^{-1}\sum_{i=j+1}^{n}\hat{\xi}_{i-j}\hat{\xi}_{i}^{\prime}
\end{align*}

\noindent where $k\left(i/h\right)$ is a kernel weighting function
where $h$ is the kernel bandwidth. 

The one-way clustered covariance estimator is defined as 
\begin{align*}
n^{-1}\sum_{j=1}^{g}\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)^{\prime}
\end{align*}

\noindent where $\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}$ is the
sum of the scores for all members in group $\mathcal{G}_{j}$ and
$g$ is the number of groups. 

If the debiased flag is used to perform a small-sample adjustment,
all estimators except the clustered covariance are rescaled by $\left(n-k\right)/n$.
The clustered covariance is rescaled by $\left(\left(n-k\right)\left(n-1\right)/n^{2}\right)\left(\left(g-1\right)/g\right)$.\footnote{This somewhat non-obvious choice is drive by Stata compatibility.}

\subsubsection*{P-values}

P-values are computes using 2-sided tests, 
\[
Pr\left(\left|z\right|>Z\right)=2-2\Phi\left(\left|z\right|\right)
\]

\noindent If the covariance estimator was debiased, a Student's t
distribution with $n-k$ degrees of freedom is used, 

\begin{align*}
Pr\left(\left|z\right|>Z\right) & =2-2t_{n-k}\left(\left|z\right|\right)
\end{align*}
where $t_{n-k}\left(\cdot\right)$ is the CDF of a Student's T distribution.

\subsubsection*{Confidence Intervals }

Confidence intervals are constructed as 
\[
CI_{i,1-\alpha}=\hat{\beta}_{i}\pm q_{\alpha/2}\times\hat{\sigma}_{\beta_{i}}
\]

\noindent where $q_{\alpha/2}$ is the $\alpha/2$ quantile of a standard
Normal distribution or a Student's t. The Student's t is used when
a debiased covariance estimator is used.

\subsubsection*{GMM estimators}

Gmm Covariance

\subsection*{To Do}

Std err, T-stats

All df's

Linear hypothesis testing

\subsection*{Post-estimation}

\subsubsection*{2SLS and LIML Post-estimation Results}

sargan

basman

wu haussman

wooldridge score

wooldridge regression

wooldridge overid

anderson rubin

basmann f

\subsubsection*{GMM Post-estimation Results}

J-stat

C-stat

\subsection*{First-stage Estimation Analysis}

First Stage Results -> partial r2, shea r2, f-stat

\subsection*{Kernel Weights and Bandwidth Selection}

Kernel weights

Optimal BW selection

\subsection*{Constant Detection}

Constant detection
\end{document}
